{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac948cc-a50b-4644-aaa2-1d276b182603",
   "metadata": {},
   "source": [
    "# Notebook 1 - Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a36f50-ec2f-42c7-bf0b-70dc2c94c958",
   "metadata": {},
   "source": [
    "## Install necessary packages to the class jupyterbook\n",
    "* Only needs to be run once per server reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e6e21-93ec-4ee6-8abc-20bb17017b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyPRISMClimate\n",
    "#!pip install hydrofunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364eb7d-1a97-42c9-833a-d51de0bf110a",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054d680-5b98-4528-bf47-46d671eef09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standards\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# geospatial tools\n",
    "import rasterio as rio\n",
    "from rasterio import plot, mask\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "# file management tools\n",
    "import glob\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "# data download tools\n",
    "from pyPRISMClimate import get_prism_dailys\n",
    "import hydrofunctions as hf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a24e2-68df-4d9a-975c-73b4021ba7d3",
   "metadata": {},
   "source": [
    "## Get gauge data from USGS\n",
    "* Download basin shapefile\n",
    "* Download streamflow (or open it if it's previously been downloaded)\n",
    "* Examine the data--plot the geometry and view the dataframe head to make sure it all looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7513ad9-bb6f-45b5-9258-b05ac0b1ca21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to download usgs stream data, basin\n",
    "def get_basin_data(gaugeID, start_date, end_date, sf_filepath, featureSource = 'nwissite'):\n",
    "    basin_url = f'https://labs.waterdata.usgs.gov/api/nldi/linked-data/{featureSource}/USGS-{gaugeID}/basin'\n",
    "    \n",
    "    # get shapefile of gauge basin\n",
    "    basin_gdf = gpd.read_file(basin_url)\n",
    "    \n",
    "    # get streamflow at gauge\n",
    "    streamflow = hf.NWIS(site=gaugeID, service='dv', start_date=start_date, end_date=end_date, file=sf_filepath)\n",
    "    streamflow_df = streamflow.df()\n",
    "    return basin_gdf, streamflow_df\n",
    "\n",
    "everson_gdf, everson_streamflow = get_basin_data(gaugeID = '12211200', start_date = '2021-11-10', end_date = '2021-12-10', sf_filepath = 'discharge_data/everson.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef03ce5-2039-4c32-a2c8-ce5a956c37ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot to check it out, make sure it meets expectations\n",
    "f, a = plt.subplots()\n",
    "everson_gdf.plot(facecolor = 'green', ax=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ac87a-230e-43e4-ab9b-87e3ab339bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "everson_streamflow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af8529-4235-476f-a52a-0799c9ef129c",
   "metadata": {},
   "source": [
    "## Download PRISM Data\n",
    "* Current default variables are precipitation and mean temperature (for snow), but can be updated in function\n",
    "* Uses gauge basin boundaries above to mask data\n",
    "* Results in netcdf files for the variables as well as corresponding data arrays\n",
    "* Preserves variable, date\n",
    "* Plot the data arrays to make sure they look good\n",
    "\n",
    "To do: \n",
    "* change return function to be more flexible in it's reference\n",
    "* make a dictionary to update variable units?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbb1b0-0eb1-49cf-a5e8-988fde635dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_weather_for_watershed(start_date, end_date, destination, boundaries, varlist = ['ppt', 'tmean']):\n",
    "    # this function will help keep track of the variable being downloaded and the date\n",
    "    def get_variable_and_date(filename):\n",
    "        filename = filename.split('_')\n",
    "        variable, date = filename[-5], pd.to_datetime(filename[-2])\n",
    "        return variable, date\n",
    "        \n",
    "    # download prism data for each requested variable\n",
    "    for var in varlist:\n",
    "        files = get_prism_dailys(var, min_date = start_date, \n",
    "                                 max_date = end_date, dest_path = destination,\n",
    "                                 keep_zip=False)\n",
    "    \n",
    "        # makes a list of the weather files downloaded by above\n",
    "        prism_file_paths = glob.glob(os.path.join(destination, '*.bil'))\n",
    "        prism_file_paths.sort()\n",
    "    \n",
    "        #empty list of datasets\n",
    "        dss = []\n",
    "    \n",
    "        for file in prism_file_paths:\n",
    "            # open file\n",
    "            weather_file = rio.open(file)\n",
    "        \n",
    "            # mask file by watershed extent\n",
    "            rio_mask_kwargs = {'filled':False, 'crop':True, 'indexes':1}\n",
    "            array, array_transform = rio.mask.mask(weather_file, everson_gdf.to_crs(weather_file.crs).geometry, \n",
    "                                               **rio_mask_kwargs)\n",
    "        \n",
    "            lon = []\n",
    "            lat = []\n",
    "        \n",
    "            # transform pixel positions to latitude and longitude for xarray coordinates using the affine transform\n",
    "            for i in range(array.shape[0]):\n",
    "                x, y = rio.transform.xy(array_transform, i, 0)\n",
    "                lon = np.append(lon, y)\n",
    "        \n",
    "            for j in range(array.shape[1]):\n",
    "                x, y = rio.transform.xy(array_transform, 0, j)\n",
    "                lat = np.append(lat, x)\n",
    "        \n",
    "            # use function to pull variable and date from filename\n",
    "            variable, date = get_variable_and_date(file)\n",
    "        \n",
    "            # turn array into xarray to accomodate more dimensions\n",
    "            ds = xr.Dataset({variable: (['lat', 'lon'], array)}, \n",
    "                                 coords = {'lat': lon, 'lon': lat})\n",
    "        \n",
    "            # add date dimension\n",
    "            ds = ds.expand_dims(dim = 'date')\n",
    "            ds.coords['date'] = ('date', [date])\n",
    "\n",
    "            # add to full list of arrays\n",
    "            dss.append(ds)\n",
    "        \n",
    "            # remove file (memory management), comment this line out if you'd like to preserve\n",
    "            os.remove(file)\n",
    "    \n",
    "        # create single dataset of all datasets for variable\n",
    "        globals()[f'ds_{variable}'] = xr.concat(dss, 'date')\n",
    "        \n",
    "        # download as a netcdf to open in other notebooks\n",
    "        out_string = os.path.join(destination, f'{variable}_{start_date}-{end_date}.nc')\n",
    "        globals()[f'ds_{variable}'].to_netcdf(path=out_string)\n",
    "    \n",
    "    # delete prism metadata files, they don't take up much space but there are a lot and they add clutter\n",
    "    for filename in glob.glob(os.path.join(destination, \"PRISM*\")):\n",
    "        os.remove(filename)\n",
    "    return ds_ppt, ds_tmean\n",
    "\n",
    "test_file_ppt, test_file_tmean = get_weather_for_watershed('2021-11-10', '2021-12-10', 'precip_data/', everson_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c2d3d-a8f1-479c-bf3a-a69aecc993be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_file_ppt['ppt'].isel(date=slice(0, 31, 1)).plot.imshow(col = 'date', col_wrap = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759d921-a8c5-412e-bed0-18f931fc64cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_file_tmean['tmean'].isel(date=slice(0, 31, 1)).plot.imshow(col = 'date', col_wrap = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0640298-22ae-47ac-b369-9b81d9b4581b",
   "metadata": {},
   "source": [
    "## DEM stuff\n",
    "To do:\n",
    "* figure out if this stays in\n",
    "* generate shapefile of boundaries to feed into above function?\n",
    "* but for parameterization purposes needs to retain cell size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba4777-ea14-4c9b-aad0-d737e098606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dem\n",
    "fork = 'nf'\n",
    "in_asc_string = f'./dem_or_shapefile/final_{fork}_30m.asc'\n",
    "out_gdal_string = f'./dem_or_shapefile/final_{fork}_30m.tif'\n",
    "EPSG_code = 26710\n",
    "dem_proj = f'EPSG:{EPSG_code}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f953405-bef9-4450-9ca0-4248ed252737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dem_src = rio.open(out_gdal_string)\n",
    "dem_array = dem_src.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f2d13-d677-4b0e-9da8-d5cc7538bf7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(out_gdal_string):\n",
    "    !gdal_translate -of \"GTiff\" -a_srs EPSG:EPSG_code $in_asc_string $out_gdal_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450eb853-32e0-408f-983b-4f9114927846",
   "metadata": {},
   "source": [
    "## SNOTEL Data\n",
    "* For snow verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f2d33-ba34-4ac8-b246-bed326b6f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get SNOTEL data\n",
    "# ulmo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a36779-ec98-4242-b9c2-b03b168e2c9e",
   "metadata": {},
   "source": [
    "## CO-OP Data\n",
    "* for precipitation verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421c4e9-3396-40b4-9b07-958dc61e2bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get infiltration data - potentially could put it together via gSSURGO or land use files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
